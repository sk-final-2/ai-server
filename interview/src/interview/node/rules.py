from langchain_core.prompts import ChatPromptTemplate
from typing import Literal
import json, re, dirtyjson
from json_repair import repair_json

# ====================================================
# ğŸ”¹ ì–¸ì–´ ê²€ì¦ / ì •ê·œí™” ìœ í‹¸
# ====================================================

_HANGUL = r"[ê°€-í£]"
_LATIN  = r"[A-Za-z]"
_CJK    = r"[\u4E00-\u9FFF\u3400-\u4DBF]"   # í•œì
_JP     = r"[\u3040-\u30FF]"

def _ratio(text: str, pattern: str) -> float:
    if not text:
        return 0.0
    total = len(re.findall(r"\S", text))
    hits  = sum(len(m) for m in re.findall(pattern, text))
    return hits / max(total, 1)

def validate_language_text(text: str, target: Literal["KOREAN", "ENGLISH"]) -> bool:
    """í…ìŠ¤íŠ¸ê°€ ì§€ì • ì–¸ì–´ ê·œì¹™ì„ ë”°ë¥´ëŠ”ì§€ ê²€ì¦"""
    hangul = _ratio(text, _HANGUL)
    latin  = _ratio(text, _LATIN)
    cjk    = _ratio(text, _CJK)
    jp     = _ratio(text, _JP)
    if target == "KOREAN":
        return (hangul >= 0.80) and ((cjk + jp + latin) <= 0.20)
    else:
        return (latin >= 0.80) and ((cjk + jp) <= 0.20)

def normalize_text(llm, text: str, target: Literal["KOREAN", "ENGLISH"], nounify: bool=False) -> str:
    """í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ë‚˜ ì˜ì–´ë¡œ ì •ê·œí™” (í† í”½ìš©ì´ë©´ ëª…ì‚¬í˜•ìœ¼ë¡œ ë³€í™˜ ê°€ëŠ¥)"""
    if target == "ENGLISH":
        sys_rule = "Use English only. Keep the meaning. Output body only."
        if nounify:
            sys_rule += " Convert into a noun phrase (not a question)."
        user = "Rewrite in English only:\n" + (text or "")
    else:
        sys_rule = "ì˜¤ì§ í•œêµ­ì–´ë§Œ ì‚¬ìš©. ì˜ë¯¸ ìœ ì§€. ë³¸ë¬¸ë§Œ ì¶œë ¥."
        if nounify:
            sys_rule += " ëª…ì‚¬êµ¬ë¡œ ë³€í™˜. ì§ˆë¬¸ë¬¸/ì¡°ì‚¬/ì–´ë¯¸ ì œê±°."
        user = "í•œêµ­ì–´ë¡œ ë‹¤ì‹œ ì“°ê¸°:\n" + (text or "")

    # í† í° ì ˆì•½: ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°
    if len(user) > 350:
        user = user[:350].rsplit(" ", 1)[0] + "..."

    try:
        resp = (
            ChatPromptTemplate.from_messages([
                ("system", sys_rule),
                ("user", "{u}")
            ]) | llm.bind(max_tokens=60, temperature=0)
        ).invoke({"u": user})
        return getattr(resp, "content", str(resp)).strip()
    except Exception:
        return text or ""


# ====================================================
# ğŸ”¹ í† í”½ ì •ê·œí™”
# ====================================================

def normalize_topic_str(text: str) -> str:
    """ë¬¸ì¥í˜• í† í”½ì„ ëª…ì‚¬í˜• í‚¤ì›Œë“œë¡œ ì •ê·œí™”"""
    t = text.strip()
    t = re.sub(r'ì—\s*(ëŒ€í•œ|ê´€í•œ|ëŒ€í•´)', ' ', t)
    t = re.sub(r'(ì´|ê°€)?\s*ìˆëŠ”ì§€$', '', t)
    t = re.sub(r'(í•˜ëŠ”ì§€|í–ˆëŠ”ì§€|ë ì§€|ë ê¹Œ)$', '', t)
    t = re.sub(r'(ì¸ê°€ìš”|ì¸ê°€|ì´ëƒ)$', '', t)
    return re.sub(r'\s+', ' ', t).strip()


# ====================================================
# ğŸ”¹ JSON íŒŒì‹± (LLM ì¶œë ¥ ë³´ì • í¬í•¨)
# ====================================================

def safe_parse_json_from_llm(content: str) -> dict:
    print("ğŸ“¨ [LLM ì‘ë‹µ ì›ë¬¸]:", content)
    
    # ë¬´ì¡°ê±´ ì´ˆê¸°í™”
    cleaned = str(content).strip().replace("```json", "").replace("```", "").strip()

    # âœ… ìë™ ê´„í˜¸ ë³´ì •
    if cleaned.count("{") > cleaned.count("}"):
        cleaned += "}" * (cleaned.count("{") - cleaned.count("}"))

    try:
        parsed = json.loads(cleaned)
        if isinstance(parsed, dict):
            print("âœ… [JSON íŒŒì‹± ì„±ê³µ]:", parsed)
            return parsed
    except Exception as e:
        print("âŒ [JSON íŒŒì‹± ì‹¤íŒ¨ - 1ì°¨]:", str(e))

    # âœ… fallback: dirtyjson
    try:
        parsed = dirtyjson.loads(cleaned)
        if isinstance(parsed, dict):
            print("âœ… [JSON íŒŒì‹± ì„±ê³µ - dirtyjson]:", parsed)
            return parsed
    except Exception as e:
        print("âŒ [JSON íŒŒì‹± ì‹¤íŒ¨ - dirtyjson]:", str(e))

    # ì‹¤íŒ¨í•˜ë©´ ë¹ˆ dict
    return {}



# ====================================================
# ğŸ”¹ ì¸í„°ë·° ê·œì¹™ ë§¤í•‘
# ====================================================

TYPE_RULE = {
    "TECHNICAL": (
        "- ì„ íƒí•œ ì§ë¬´ì˜ ëŒ€í‘œ ê¸°ìˆ  ìŠ¤íƒ, ì•Œê³ ë¦¬ì¦˜, ë„êµ¬ì— ê¸°ë°˜í•œ ì§ˆë¬¸ë§Œ ìƒì„±í•  ê²ƒ\n"
        "- ì§ˆë¬¸ í˜•ì‹ì€ ë‹¤ì–‘í•˜ê²Œ ì„ì„ ê²ƒ: (ì„¤ëª…í˜•, ë¹„êµí˜•, ì ìš©í˜•, ëŒ€ì•ˆí˜•)\n"
        "- ë‹¨ìˆœ ì •ì˜ ì•”ê¸° ì§ˆë¬¸ ëŒ€ì‹ , ë°˜ë“œì‹œ ìƒí™©Â·ì„ íƒ ì´ìœ Â·ëŒ€ì•ˆ ì¤‘ í•˜ë‚˜ ì´ìƒ í¬í•¨\n"
        "- ì‹¤ë¬´ ì‹œë®¬ë ˆì´ì…˜ í¬í•¨ ê°€ëŠ¥: ë°°í¬/ì¥ì• /ì„±ëŠ¥ ì €í•˜ ë“± ì‹¤ì œ ì—…ë¬´ ë§¥ë½ ê°€ì •\n"
        "- ì§ì „ ë‹µë³€ì˜ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ê·¸ ë¶€ë¶„ì„ ë” ê¹Šì´ íŒŒê³ ë“œëŠ” í›„ì† ì§ˆë¬¸ì„ 30% í™•ë¥ ë¡œ ìƒì„±\n"
        "- ê¸ˆì§€: ì¸ì„±/ë™ê¸°/ê°€ì¹˜ê´€/ë¬¸í™” ì í•© ê´€ë ¨ ì§ˆë¬¸"
    ),
    "PERSONALITY": (
        "- ì§€ì›ìì˜ í–‰ë™, ë™ê¸°, í˜‘ì—…, ê°ˆë“± í•´ê²° ê²½í—˜ì„ ì‚¬ë¡€ ì¤‘ì‹¬ìœ¼ë¡œ ë¬»ëŠ”ë‹¤ (STAR êµ¬ì¡° ìœ ë„)\n"
        "- ì§ˆë¬¸ í˜•ì‹ì€ ë‹¤ì–‘í•˜ê²Œ ì„ì„ ê²ƒ: (ê²½í—˜ ì„¤ëª…, ìƒí™© ëŒ€ì²˜, í–‰ë™ ì´ìœ , ê²°ê³¼ ë°˜ì„±)\n"
        "- ì‹¤ë¬´ ì‹œë®¬ë ˆì´ì…˜ í¬í•¨ ê°€ëŠ¥: ë§ˆê° ì§€ì—°, í˜‘ì—… ê°ˆë“±, ì˜ì‚¬ì†Œí†µ ë¬¸ì œ ë“± ì‹¤ì œ íŒ€ ìƒí™© ê°€ì •\n"
        "- ì§ì „ ë‹µë³€ì—ì„œ ì–¸ê¸‰ëœ ê²½í—˜/ì‚¬ë¡€ë¥¼ ë” ê¹Šì´ ìºë¬»ëŠ” follow-up ì§ˆë¬¸ì„ 30% í™•ë¥ ë¡œ ìƒì„±\n"
        "- ê¸ˆì§€: ì¶”ìƒì  ê°€ì¹˜ê´€, ì¥ë˜ì„±, ì¥ë‹¨ì , í¬ê´„ì  ìê¸°í‰ê°€"
    ),
    "MIXED": ( 
        "- ê¸°ìˆ ê³¼ ì¸ì„± ì§ˆë¬¸ì„ ê· í˜• ìˆê²Œ ì„ë˜ ë™ì¼ ìœ í˜•ë§Œ ë°˜ë³µí•˜ì§€ ë§ ê²ƒ."
        "- ê¸°ìˆ  ì§ˆë¬¸ì€ ìœ„ì˜ TECHNICAL ê·œì¹™ì„, ì¸ì„± ì§ˆë¬¸ì€ PERSONALITY ê·œì¹™ì„ ê°ê° ë”°ë¥¼ ê²ƒ."
    )
}

LEVEL_RULE = {
    "í•˜": (
        "- ê¸°ë³¸ ê°œë…ì´ë‚˜ ë‹¨ìˆœ ê²½í—˜ íšŒìƒ ìˆ˜ì¤€ ì§ˆë¬¸\n"
        "- ì˜ˆì‹œ: 'ìµœê·¼ì— ê³µë¶€í•œ ê¸°ìˆ  ì¤‘ ê°€ì¥ í¥ë¯¸ë¡œì› ë˜ ê²ƒì€ ë¬´ì—‡ì¸ê°€ìš”?'\n"
        "- ë¶€ë‹´ì„ ì¤„ì´ê³ , ì§ë¬´ ê¸°ë³¸ê¸°ë¥¼ í™•ì¸í•˜ëŠ” ì§ˆë¬¸"
    ),
    "ì¤‘": (
        "- ì‹¤ì œ ìƒí™© ì ìš©, íŠ¸ë ˆì´ë“œì˜¤í”„, ì„ íƒ ì´ìœ ë¥¼ ë¬»ëŠ” ì§ˆë¬¸\n"
        "- ë°˜ë“œì‹œ í•˜ë‚˜ ì´ìƒì˜ êµ¬ì²´ì  ì‚¬ë¡€ë¥¼ ì „ì œë¡œ ì§ˆë¬¸\n"
        "- ì˜ˆì‹œ: 'íŒ€ í”„ë¡œì íŠ¸ì—ì„œ ë°ì´í„°ë² ì´ìŠ¤ ì„ íƒ ê³¼ì •ì—ì„œ ì–´ë–¤ ê¸°ì¤€ì„ ê³ ë ¤í•˜ì…¨ë‚˜ìš”?'"
    ),
    "ìƒ": (
        "- ì‹¬ì¸µì , ì••ë°• ë©´ì ‘ ìŠ¤íƒ€ì¼ ê°€ëŠ¥ (20~30% í™•ë¥ ë¡œ ì••ë°•/ê³¡ì„  ì§ˆë¬¸ í—ˆìš©)\n"
        "- í•œê³„Â·ì‹¤íŒ¨Â·ë¦¬ìŠ¤í¬ ê´€ë¦¬Â·ëŒ€ì•ˆ ë¹„êµë¥¼ ë¬»ëŠ”ë‹¤\n"
        "- ì˜ˆì‹œ: 'í”„ë¡œì íŠ¸ ì„±ëŠ¥ì´ ê¸°ëŒ€ ì´í•˜ì˜€ì„ ë•Œ ë³¸ì¸ì˜ ì±…ì„ì€ ë¬´ì—‡ì´ë¼ê³  ìƒê°í•˜ì‹œë‚˜ìš”?'"
    )
}

LANG_RULE = {
    "KOREAN": (
        "- ì¶œë ¥ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ ì§ˆë¬¸ í•˜ë‚˜ë¿. "
        "- í‰ì„œë¬¸(~ë‹¤, ~ìˆë‹¤) ê¸ˆì§€, ì§ˆë¬¸ ì–´ë¯¸(~ë‚˜ìš”?, ~ìŠµë‹ˆê¹Œ?) í•„ìˆ˜."
    ),
    "ENGLISH": (
        "- Output must be in English only. "
        "- Must be a single interview question ending with '?'. "
        "- No prefaces, no numbering, no explanations."
    )
}


# ====================================================
# ğŸ”¹ system_rule ìƒì„±ê¸°
# ====================================================

def system_rule(state) -> str:
    """ë©´ì ‘ ìœ í˜• + ë‚œì´ë„ + ì–¸ì–´ì— ë§ëŠ” system prompt ìƒì„±"""
    language = getattr(state, "language", "KOREAN")
    interviewType = getattr(state, "interviewType", "MIXED")
    career = getattr(state, "career", "ì‹ ì…")
    level = getattr(state, "level", "ì¤‘")

    base = "ë„ˆëŠ” ë©´ì ‘ê´€ì´ë‹¤. " if language == "KOREAN" else "You are an interviewer. "
    base += "- ì¶œë ¥ì€ ë°˜ë“œì‹œ ì§ˆë¬¸ í•˜ë‚˜ë¿ì´ë‹¤. ë‹µë³€, í•´ì„¤, ì„¤ëª…, ë©”íƒ€ ë¬¸êµ¬ ê¸ˆì§€. "
    base += TYPE_RULE.get(interviewType, "")
    base += LEVEL_RULE.get(level, "")
    base += LANG_RULE.get(language, "")

    # ê²½ë ¥ êµ¬ë¶„
    if career == "ì‹ ì…":
        base += ( 
            " ì§€ì›ìëŠ” ì‹ ì…ì´ë‹¤. ë”°ë¼ì„œ ì§ˆë¬¸ì€ í•™ìŠµ íƒœë„, ìƒˆë¡œìš´ ê¸°ìˆ /ì§€ì‹ ìŠµë“ ê²½í—˜, "
            "íŒ€ ë‚´ ì ì‘ë ¥, í˜‘ì—… ë°©ì‹, ì‹¤íŒ¨ í›„ ê·¹ë³µ ì‚¬ë¡€ì— ì´ˆì ì„ ë§ì¶œ ê²ƒ. "
            "ì¶”ìƒì  ì„±ì¥ ê°€ëŠ¥ì„± ëŒ€ì‹  ì‹¤ì œ ê²½í—˜ ê¸°ë°˜ ì§ˆë¬¸ì„ ìƒì„±í•´ì•¼ í•œë‹¤."
            "ì§ˆë¬¸ì€ ì‹¤ì œ ê¸°ì—… ë©´ì ‘ê´€ì´ ì‹ ì… ì§€ì›ìì—ê²Œ í•  ë²•í•œ, ì–´ë µê³  ì¶”ìƒì ì¸ í‘œí˜„ë³´ë‹¤ëŠ” ê°„ë‹¨í•˜ê³  ì§ê´€ì ì¸ í‘œí˜„ìœ¼ë¡œ ìƒì„±í•œë‹¤. ì˜ˆ: 'í–¥í›„ ëª©í‘œëŠ” ë¬´ì—‡ì¸ê°€ìš”?', '5ë…„ ë’¤ ì–´ë–¤ ëª¨ìŠµì´ ë˜ê³  ì‹¶ë‚˜ìš”?'"
        )
        
    elif career in ["ê²½ë ¥", "ê²½ë ¥ì§"]:
        base += (
             " ì§€ì›ìëŠ” ê²½ë ¥ì§ì´ë‹¤. ë”°ë¼ì„œ ì§ˆë¬¸ì€ ì‹¤ë¬´ ì„±ê³¼, ì˜ì‚¬ê²°ì • ê³¼ì •, "
             "ë¦¬ë”ì‹­/ë©˜í† ë§ ê²½í—˜, ê°ˆë“± í•´ê²°, ì¡°ì§ ê¸°ì—¬ì™€ ì±…ì„ì— ì´ˆì ì„ ë§ì¶œ ê²ƒ."
             "ê°€ëŠ¥í•˜ë‹¤ë©´ ìˆ˜ì¹˜ë‚˜ êµ¬ì²´ì  ì§€í‘œë¥¼ ëŒì–´ë‚¼ ìˆ˜ ìˆëŠ” ì§ˆë¬¸ì„ ìƒì„±í•´ì•¼ í•œë‹¤."
    )

    return base


# ====================================================
# ğŸ”¹ ì§ˆë¬¸ í›„ì²˜ë¦¬
# ====================================================

def validate_question(q: str, lang: str = "KOREAN") -> bool:
    """ì§ˆë¬¸ì´ ìì—°ìŠ¤ëŸ¬ìš´ ì–´ë¯¸ ê·œì¹™ì„ ë”°ë¥´ëŠ”ì§€ í™•ì¸"""
    q = q.strip()
    if lang == "KOREAN":
        endings = ["ë‚˜ìš”?", "ìŠµë‹ˆê¹Œ?", "í• ê¹Œìš”?", "ìˆë‚˜ìš”?", "ìˆìŠµë‹ˆê¹Œ?", "ë¬´ì—‡ì¸ê°€ìš”?", "ì–´ë–»ê²Œ ìƒê°í•˜ë‚˜ìš”?"]
        return any(q.endswith(end) for end in endings)
    else:
        return q.endswith("?") and q.lower().split()[0] in [
            "what", "why", "how", "when", "where",
            "do", "does", "did", "can", "could", "would"
        ]

def clean_question(q: str) -> str:
    """ë¶ˆí•„ìš”í•œ ì ‘ë‘ì‚¬, ë©”íƒ€ì„¤ëª… ì œê±°"""
    q = q.strip()
    q = re.sub(r'^(?:\d+|Q\d+|ì§ˆë¬¸)[:\.\-\s]*', '', q, flags=re.IGNORECASE)  # ë²ˆí˜¸ ì œê±°
    q = re.sub(r'.*ì— ëŒ€í•œ ì§ˆë¬¸(ì…ë‹ˆë‹¤|ì…ë‹ˆë‹¤\.)', '', q)                   # "ì— ëŒ€í•œ ì§ˆë¬¸ì…ë‹ˆë‹¤" ì œê±°
    q = q.strip('"â€œâ€')
    return q
