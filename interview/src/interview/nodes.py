from interview.model import InterviewState
from langchain_core.prompts import ChatPromptTemplate
from interview.chroma_qa import get_similar_qa
from langchain_openai import ChatOpenAI
import os
from dotenv import load_dotenv

load_dotenv("src/interview/.env")

llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0.7,
    api_key=os.getenv("OPENAI_API_KEY")
)

# âœ… ëª¨ë“  ë…¸ë“œ í•¨ìˆ˜ëŠ” dictë¥¼ ë°›ê³  dictë¥¼ ë°˜í™˜í•˜ê²Œ ë³€ê²½
def first_question_node(state: dict) -> dict:
    state = InterviewState(**state)
    print("ğŸ¯ [first_question_node] ì²« ì§ˆë¬¸ ìƒì„±")
    print("ğŸ“„ [ì´ë ¥ì„œ í…ìŠ¤íŠ¸]:", state.text[:200], "...")
    print("ğŸ’¼ [ì§€ì› ì§ë¬´]:", state.job)

    prompt = ChatPromptTemplate.from_messages([
        ("system", """
ë‹¹ì‹ ì€ ë©´ì ‘ê´€ì…ë‹ˆë‹¤. ì•„ë˜ ì§€ì›ìì˜ ìê¸°ì†Œê°œì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ
ë©´ì ‘ì—ì„œ ì‹œì‘í•  ì²« ë²ˆì§¸ ì§ˆë¬¸ì„ í•œêµ­ì–´ë¡œë§Œ ìì—°ìŠ¤ëŸ½ê²Œ ìƒì„±í•˜ì„¸ìš”.
- ë„ˆë¬´ ê´‘ë²”ìœ„í•˜ì§€ ì•Šê²Œ, í•œ ë¬¸ì¥ìœ¼ë¡œ ì§ˆë¬¸ í˜•íƒœë¡œ ëë‚´ì„¸ìš”.
- ì˜ˆì‹œ: \"ìê¸°ì†Œê°œì„œì— ë‚˜ì˜¨ í”„ë¡œì íŠ¸ ì¤‘ ê°€ì¥ ê¸°ì–µì— ë‚¨ëŠ” ê²½í—˜ì€ ë¬´ì—‡ì¸ê°€ìš”?\"
ì§€ì›ìì˜ ìê¸°ì†Œê°œì„œ:
{resume}
""")
    ])

    chain = prompt | llm
    response = chain.invoke({"resume": state.text})

    question = response.content.strip() if hasattr(response, "content") else str(response).strip()

    if not question:
        raise ValueError("ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨")

    state.questions.append(question)
    state.step += 1
    return state.model_dump()

def answer_node(state: dict) -> dict:
    state = InterviewState(**state)
    print("âœï¸ [answer_node] ì‚¬ìš©ì ë‹µë³€ ìˆ˜ì§‘")
    state.step += 1
    return state.model_dump()

def analyze_node(state: dict) -> dict:
    state = InterviewState(**state)
    print("ğŸ” [analyze_node] ë‹µë³€ ë¶„ì„")
    answer = state.answers[-1] if state.answers else ""

    prompt = ChatPromptTemplate.from_messages([
        ("system", """
ë„ˆëŠ” ë©´ì ‘ í‰ê°€ìì…ë‹ˆë‹¤. ì•„ë˜ì˜ ë‹µë³€ì„ ë¶„ì„í•´ì„œ 'ì˜í•œ ì ', 'ê°œì„ ì´ í•„ìš”í•œ ì ', 'ì ìˆ˜(0~100)'ë¥¼ ê°ê° í•˜ë‚˜ì”© ë„ì¶œí•˜ì„¸ìš”.
í˜•ì‹ì€ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”.
{
  "good": "ì˜í•œ ì ",
  "bad": "ê°œì„ ì´ í•„ìš”í•œ ì ",
  "score": ìˆ«ì
}
"""),
        ("human", "{answer}")
    ])

    chain = prompt | llm
    analysis = chain.invoke({"answer": answer}).content
    state.last_analysis = {"comment": analysis}
    state.step += 1
    return state.model_dump()

def next_question_node(state: dict) -> dict:
    state = InterviewState(**state)
    print("â¡ï¸ [next_question_node] ë‹¤ìŒ ì§ˆë¬¸ ìƒì„±")

    if len(state.questions) >= 3:
        state.is_finished = True
    else:
        # ìœ ì‚¬ ì§ˆë¬¸ ê¸°ë°˜ ì¶”ë¡  (ì„ íƒ)
        previous_answer = state.answers[-1] if state.answers else ""
        next_q = get_similar_qa(previous_answer)
        state.questions.append(next_q)

    state.step += 1
    return state.model_dump()