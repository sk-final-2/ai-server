from interview.model import InterviewState
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
import os
import json
from dotenv import load_dotenv

load_dotenv(dotenv_path="src/interview/.env")

llm = ChatOpenAI(
    openai_api_key=os.getenv("OPENAI_API_KEY"),
    base_url="https://api.groq.com/openai/v1",
    model="llama3-8b-8192",
    temperature=0.7
)

def safe_parse_json_from_llm(content: str) -> dict:
    print("ğŸ“¨ [LLM ì‘ë‹µ ì›ë¬¸ - ë‹¤ì‹œ í™•ì¸]:", content)

    # JSON í¬ë§·ì„ ê°ì‹¸ëŠ” ë¶ˆí•„ìš”í•œ êµ¬ë¬¸ ì œê±°
    cleaned = content.strip().replace("```json", "").replace("```", "").strip()

    try:
        return json.loads(cleaned)
    except json.JSONDecodeError as e:
        print("âŒ JSON ë””ì½”ë”© ì‹¤íŒ¨:", e)
        raise
    
def first_question_node(state: InterviewState) -> InterviewState:
    print("ğŸ¯ [first_question_node] ì²« ì§ˆë¬¸ ìƒì„±")
    print("ğŸ“„ [ì´ë ¥ì„œ í…ìŠ¤íŠ¸]:", state.text[:200], "...")  # ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ì„œ ì¶œë ¥
    print("ğŸ’¼ [ì§€ì› ì§ë¬´]:", state.job)
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", """
ë‹¹ì‹ ì€ ë©´ì ‘ê´€ì…ë‹ˆë‹¤. ì•„ë˜ ì§€ì›ìì˜ ìê¸°ì†Œê°œì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ
ë©´ì ‘ì—ì„œ ì‹œì‘í•  ì²« ë²ˆì§¸ ì§ˆë¬¸ì„ í•œêµ­ì–´ë¡œë§Œ ìì—°ìŠ¤ëŸ½ê²Œ ìƒì„±í•˜ì„¸ìš”.

- ë„ˆë¬´ ê´‘ë²”ìœ„í•˜ì§€ ì•Šê²Œ, í•œ ë¬¸ì¥ìœ¼ë¡œ ì§ˆë¬¸ í˜•íƒœë¡œ ëë‚´ì„¸ìš”.
- ì˜ˆì‹œ: "ìê¸°ì†Œê°œì„œì— ë‚˜ì˜¨ í”„ë¡œì íŠ¸ ì¤‘ ê°€ì¥ ê¸°ì–µì— ë‚¨ëŠ” ê²½í—˜ì€ ë¬´ì—‡ì¸ê°€ìš”?"

ì§€ì›ìì˜ ìê¸°ì†Œê°œì„œ:
{resume}
""")
    ])

    chain = prompt | llm

    last_answer = state.answers[-1] if state.answers else ""

    try:
        response = chain.invoke({
            "resume": state.text,
            "job": state.job,
            "answer": last_answer
        })
        print("ğŸ§  [LLM ì‘ë‹µ]:", response)
        question = response.content
        if not question:
            print("â— [ê²½ê³ ] ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: ë¹ˆ ì‘ë‹µ")
            raise ValueError("LLM ì‘ë‹µì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        print("âŒ [LLM í˜¸ì¶œ ì‹¤íŒ¨]:", str(e))
        raise e  # ì´ê±¸ ë°˜ë“œì‹œ ë‹¤ì‹œ ë˜ì ¸ì•¼ FastAPIì—ì„œ 500ìœ¼ë¡œ ê¸°ë¡ë¨

    state.questions.append(question)
    return state 

def answer_node(state: InterviewState) -> InterviewState:
    print("ğŸ—£ï¸ [answer_node] ë‹µë³€ ìˆ˜ì§‘ ì™„ë£Œ")
    if state.last_answer:
        state.answers.append(state.last_answer)
    else:
        print("âš ï¸ last_answerê°€ ë¹„ì–´ ìˆìŒ")
    state.step += 1
    return state

def analyze_node(state: InterviewState) -> InterviewState:
    print("ğŸ” [analyze_node] ë‹µë³€ ë¶„ì„")
    answer = state.answers[-1] if state.answers else ""

    prompt = ChatPromptTemplate.from_messages([
        ("system", """
ë„ˆëŠ” ë©´ì ‘ í‰ê°€ìì…ë‹ˆë‹¤. ì•„ë˜ì˜ ë‹µë³€ì„ ë¶„ì„í•´ì„œ 'ì˜í•œ ì ', 'ê°œì„ ì´ í•„ìš”í•œ ì ', 'ì ìˆ˜(0~100)'ë¥¼ ê°ê° í•˜ë‚˜ì”© ë„ì¶œí•˜ì„¸ìš”.
í˜•ì‹ì€ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”.

{{
  "good": "ì˜í•œ ì ",
  "bad": "ê°œì„ ì´ í•„ìš”í•œ ì ",
  "score": ìˆ«ì
}}
"""),
        ("human", "{answer}")
    ])

    chain = prompt | llm
    response = chain.invoke({"answer": answer}).content
    print("ğŸ“¨ [LLM ì‘ë‹µ]:", response)

    try:
        # JSON í˜•ì‹ìœ¼ë¡œ ì•ˆì „í•˜ê²Œ íŒŒì‹±
        analysis = safe_parse_json_from_llm(response)

        # ê°œë³„ í•„ë“œì— ì§ì ‘ ì €ì¥
        state.interview_answer_good = analysis.get("good", "")
        state.interview_answer_bad = analysis.get("bad", "")
        state.score = analysis.get("score", 0)

        # ì „ì²´ ë¶„ì„ ê²°ê³¼ë„ ì €ì¥ (ì„ íƒ)
        state.last_analysis = analysis

    except Exception as e:
        print("âŒ JSON íŒŒì‹± ì‹¤íŒ¨:", e)
        # ì˜ˆì™¸ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ ì„¤ì •
        state.interview_answer_good = ""
        state.interview_answer_bad = ""
        state.score = 0
        state.last_analysis = {"error": str(e), "raw": response}

    return state

def next_question_node(state: InterviewState) -> InterviewState:
    print("â¡ï¸ [next_question_node] ë‹¤ìŒ ì§ˆë¬¸ ìƒì„±")
    if len(state.questions) >= 3:
        state.is_finished = True
    else:
        last_answer = state.answers[-1] if state.answers else ""
        prompt = ChatPromptTemplate.from_messages([
    ("system", """
ë‹¹ì‹ ì€ ë©´ì ‘ê´€ì…ë‹ˆë‹¤. ì§€ì›ìì˜ ì´ì „ ë‹µë³€ê³¼ ìê¸°ì†Œê°œì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ê¼¬ë¦¬ì§ˆë¬¸ì„ 1ê°œ ìƒì„±í•˜ì„¸ìš”.
í˜•ì‹ì€ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ë¬¸ì¥ í•˜ë‚˜ë¡œ ì¶œë ¥í•˜ê³ , ì§ˆë¬¸ í˜•ì‹ìœ¼ë¡œ ëë‚´ì„¸ìš”.
ì˜ˆ: "ê·¸ ê²½í—˜ì—ì„œ ê°€ì¥ í˜ë“¤ì—ˆë˜ ì ì€ ë¬´ì—‡ì¸ê°€ìš”?"
"""),
    ("human", "ë‹µë³€: {answer}\nìê¸°ì†Œê°œì„œ: {resume}")
])
        chain = prompt | llm
        question = chain.invoke({"answer": last_answer, "resume": state.resume}).content
        state.questions.append(question)
    state.step += 1
    return state

def end_node(state: InterviewState) -> InterviewState:
    print("ğŸ [end_node] ë©´ì ‘ ì¢…ë£Œ")
    return state